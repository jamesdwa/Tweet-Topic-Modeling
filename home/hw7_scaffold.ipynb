{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 7: Twitter Topic Modeling with Non-negative Matrix Factorization.\n",
                "\n",
                "*Content/Trigger Warning: This assignment uses real-world data from Twitter, but that comes at the risk of our dataset containing tweets about sensitive or triggering topics.The tweets in this dataset do not reflect the views of the CSE/STAT 416 course staff. We have tried our best to remove some of the negative content but can only do so much when there are over 100k tweets in our dataset.This assignment should be doable without having to dig into the tweets present and reading about content you don't want to, but we wanted to give you a heads up just in case you do see something you find offensive.*\n",
                "\n",
                "This week, we will use the techniques for recommender systems in an unexpected way to help us model topics found on Twitter. In this homework you will practice extracting topics from tweets using matrix factorization. This method assumes every tweet is a combination of several topics weighted by their prevailance in the text. This approach in fact finds a low-dimensional representation of the tweets (through the topic weights).\n",
                "\n",
                "For this assignment, we will be working with tweets about the pandemic from over two years ago when the pandemic recently entered our lives. The dataset is obtained from [Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets-late-april?select=2020-04-30+Coronavirus+Tweets.CSV) and the preprocessing we have done followed the steps [here](https://www.kaggle.com/satanizer/covid-19-tweets-analysis). For computational speed we will analyze a dataset from one day: April 30, 2020. We encourage you to explore this dataset further and see how topics change over time.\n",
                "\n",
                "Fill in the cells provided marked `TODO` with code to answer the questions. **Unless otherwise noted, every answer you submit should have code that clearly shows the answer in the output.** Answers submitted that do not have associated code that shows the answer may not be accepted for credit. \n",
                "\n",
                "\u003e Copyright ¬©2023 Valentina Staneva Hunter Schafer.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Spring Quarter 2024 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
                "\n",
                "---\n",
                "\n",
                "# Setup\n",
                "\n",
                "## Data Loading\n",
                "\n",
                "First let's read the dataset into a data frame and have a look what is there."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "text = pd.read_csv('tweets-2020-4-30.csv')\n",
                "np.random.seed(416)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_load_data) ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = text.fillna('') # some rows are nan so replace with empty string\n",
                "text.tail()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Note: Some preprocessing\n",
                "\n",
                "The dataset you have just loaded was actually pre-processed by us. We briefly describe the steps handled already just so you know that there are usually some extra things that need to be done with text data. We show all the code below if you are curious, but you do not need to fully understand these steps.\n",
                "\n",
                "* Removed tweets not in English. This is a tricky modeling choice, but one that is pretty common for simplicity and accuracy. Like when discussing bias, a better choice would probably to build up separate models for each language. \n",
                "* Removed URLs from tweets (not relevant to analysis)\n",
                "* Make all text lower-case\n",
                "* Remove all punctuation\n",
                "* Remove stop-words (e.g., \"a\", \"the\", \"to\") using [NLTK](https://www.nltk.org/).\n",
                "* Also remove some too frequent terms related to COVID that end up skewing the analysis.\n",
                "\n",
                "The code for these steps was shown below. The original dataset had extra columns other than just text.\n",
                "\n",
                "```python\n",
                "# select tweets in English\n",
                "text = data['text'][data['lang']=='en']\n",
                "\n",
                "# remove URL links\n",
                "text = text.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
                "\n",
                "# make lower case\n",
                "text = text.str.lower()\n",
                "\n",
                "# remove punctuation\n",
                "text = text.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
                "\n",
                "# remove stopwords and common COVID terms\n",
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "stop_words.update(['#coronavirus', '#coronavirusoutbreak', \n",
                "                   '#coronavirusPandemic', '#covid19', '#covid_19', \n",
                "                   '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', \n",
                "                   'covid19','covid-19', 'covid„Éº19'])\n",
                "\n",
                "def remove_stopwords(tweet):\n",
                "    words = tweet.split()\n",
                "    words = [word for word in words if word not in stop_words]\n",
                "    return ' '.join(words)  # Trick to make string separated by spaces\n",
                "\n",
                "text = text.apply(remove_stopwords)\n",
                "```\n",
                "\n",
                "## TF-IDF Matrix\n",
                "\n",
                "Remember that matrix factorization methods work on matrices of numbers not text so we need to convert the text into a meaningful numeric representation.\n",
                "\n",
                "Earlier we discussed the Term Frequency-Inverse Document Frequency as a good way to do that since it defines a word weight vector for each document by accounting for the most popular words such as `the` or `a`.  We can extract it using `scikit-learn`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "# create TF-IDF matrix\n",
                "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
                "tf_idf = vectorizer.fit_transform(text['text'])\n",
                "\n",
                "# exctract also the words so that we know which feature corresponds to which word\n",
                "feature_names = vectorizer.get_feature_names_out()\n",
                "\n",
                "# check out the shape\n",
                "tf_idf.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 1** Counts\n",
                "\n",
                "Make two variables `num_tweets` and `num_words` that store the number of tweets in our dataset and number of words in our analysis respectively. Use the number of words as the number of features after doing the TF-IDF computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q1_counts) ###\n",
                "\n",
                "# TODO compute num_tweets and num_words\n",
                "num_tweets = tf_idf.shape[0]\n",
                "num_words = tf_idf.shape[1]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modeling Tweets with Topics\n",
                "\n",
                "We will use a particular technique similar to matrix factorization for recommendation to help us model tweets. In particular we will use a model called Non-negative Matrix Decomposition to help us discover topics.\n",
                "\n",
                "You might be wondering how we can use an approach we taught for recommender systems to model tweets, when there is no notion of recommending a tweet. The idea is to try to create two matrices to describe \"Tweet factors\" and \"Word factors\" that will hopefully correspond to distinct topics of discussion. Just like with matrix factorization for recommendation, our hope is that each factor corresponds to a semantically meaningful topic.\n",
                "\n",
                "### üîç **Question 2** NMF\n",
                "\n",
                "We will use the [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) method from `scikit-learn` to extract the topics. Part of this assignment will be reading the documenation for this part of the `sklearn` library.\n",
                "\n",
                "\n",
                "Set up an NMF model with 5 components and fit it to our TF-IDF data. Use the `fit_transform` as shown in the example for the documentation above to both fit the NMF model and transform our tweet data in one step.\n",
                "\n",
                "When creating the model, you will want to use the following hyperparameters to ensure you get the same results as us:\n",
                "* `init='nndsvd'`\n",
                "* `n_components=5`\n",
                "\n",
                "When fitting the model, we will fit it on our TF-IDF data which is a matrix of the shape `(num_tweets, num_words)`.\n",
                "\n",
                "Save your model in a variable called `nmf` and the projected tweets in a variable called `tweets_projected`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q2_nmf) ###\n",
                "\n",
                "from sklearn.decomposition import NMF\n",
                "\n",
                "# TODO create and fit the model and transform our data\n",
                "nmf = NMF(n_components=5, init='nndsvd')\n",
                "tweets_projected = nmf.fit_transform(tf_idf)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 3** Inspecting Components\n",
                "\n",
                "The topics are stored within the object `nmf.components_`. Investigate this matrix and the `tweets_projected` matrices by printing their values and their shapes. Make sure you undertsand why each one has the shape it does.\n",
                "\n",
                "Looking at the `nmf.components_` field, does it correspond to the \"Tweet factors\" or \"Word factors\" in the terminology of matrix factorization? Save your answer as a string in a variable called `q3`. \n",
                "\n",
                "* If you think the answer is Tweet factors, write `q3 = 'tweet'`\n",
                "* If you think the answer is Word factors, write `  q3 = 'word'`\n",
                "\n",
                "**Note: The result for this test will be hidden on both EdStem and Gradescope. Please think carefully about your selection.**\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q3_components) ###\n",
                "\n",
                "# TODO define q3 = ...\n",
                "q3 = 'word'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analyzing Topics\n",
                "\n",
                "We are now interested in inspecting each topic to find the most prevelant or meaningful words for that topic. We'll consider the words with the highest weights for a topic in NMF model to be the most important words for that topic. Recall that the words themselves are stored in a variable called `feature_names`.\n",
                "\n",
                "### üîç **Question 4** Small Example\n",
                "Before trying to investigate the values in the real data, let's do a small example first to explore how this can be done. You can use the [`argsort()`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) to get a list of array indices sorted by the values at those indices; this is useful when you want to use the ordered indices for another purpose. \n",
                "\n",
                "In the cell below, we have defined variables `small_words` and `small_weights` that correspond to a made-up example of words with weights from a single topic in NMF. To be specific, the word at index `i` in `small_words` will have weight `small_weights[i]`. You should write code in the cell below to make a new variable `sorted_small_words` that stores a `list` of the words of `small_words` but in sorted order from *largest weight* to *smallest weight*.\n",
                "\n",
                "\n",
                "*Notes*:\n",
                "* Pay special attention to the sort order specified by `argsort`. If you need to reverse an `numpy` array stored in a variable `a`, you can write `np.flip(a)` ([np.flip documentation](https://numpy.org/doc/stable/reference/generated/numpy.flip.html)). \n",
                "* Like normal, you should not hard code the answer to this problem but write code to make it work (even though you could easily do this example by just writing out the values). We want you to practice this now since in the next few problems you will have to do this on the real-data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q4_sorted_small_words) ###\n",
                "\n",
                "small_words = ['dogs', 'cats', 'axolotl']\n",
                "small_weights = np.array([1, 4, 2])\n",
                "\n",
                "#TODO Write code to make sorted_small_words as described above\n",
                "sorted_indices = small_weights.argsort()[::-1]\n",
                "\n",
                "sorted_small_words = np.array(small_words)[sorted_indices]\n",
                "sorted_small_words"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 5** Words from Topic\n",
                "We will now generalize the code you wrote for the last section to work on our real dataset.\n",
                "\n",
                "Write a function `words_from_topic` to extract an ordered list of words in a topic (highest weight first). Please see the documentation provided in the starter function to see a description of the parameters and return.\n",
                "\n",
                "*Note*: Your solution should look very similar to the last question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q5_words_from_topic) ###\n",
                "\n",
                "def words_from_topic(topic, feature_names):\n",
                "  \"\"\"\n",
                "  Sorts the words by their weight in the given topic from largest to smallest.\n",
                "  topic and feature_names should have the same number of entries.\n",
                "\n",
                "  Args:\n",
                "  - topic (np.array): A numpy array with one entry per word that shows the weight in this topic.\n",
                "  - feature_names (list): A list of words that each entry in topic corresponds to\n",
                "\n",
                "  Returns:\n",
                "  - A list of words in feature_names sorted by weight in topic from largest to smallest. \n",
                "  \"\"\"\n",
                "\n",
                "  # TODO implement this function\n",
                "  sorted_indices = topic.argsort()[::-1]\n",
                "\n",
                "  sorted_words = np.array(feature_names)[sorted_indices]\n",
                "\n",
                "  return sorted_words.tolist()\n",
                "  "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have implemented the function above,  you should be able to run the cell below that uses your function to print out the top 10 words in each topic.\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_top_words(components, feature_names, n_top_words):\n",
                "    \"\"\" \n",
                "    print_top_words prints the first n_top_words for each topic in components\n",
                "    \"\"\"\n",
                "    for topic_index, topic in enumerate(components):\n",
                "        ordered_words = words_from_topic(topic, feature_names)\n",
                "        top_words = ', '.join(ordered_words[:n_top_words])\n",
                "        print(f'Topic: #{topic_index}: {top_words}')\n",
                "\n",
                "print_top_words(nmf.components_, feature_names, 10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Investigating Tweet\n",
                "Next let's look at a specific tweet (index 40151) and the individual contributions of the topics. The cell below prints the text of the original tweet and then the value of the tweet after being transformed by our NMF."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "index = 40151\n",
                "print(text.iloc[index]['text'])\n",
                "print(tweets_projected[index])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 6** Looking at a tweet\n",
                "\n",
                "Look at the topic values for the tweet above. Which topic (Topic #0 to Topic #4) is it most  associated with? Save your answer in a variable called `q6`.\n",
                "\n",
                "For this problem, you can hard-code your answer as a number. For example, if you look at the result and believe it is most associated with Topic 0, you could write `q6 = 0`.\n",
                "\n",
                "Does this tweet make sense to be grouped in a topic with the words shown in the topic word lists you printed in the last problem?\n",
                "\n",
                "**Note: The result for this test will be hidden on both EdStem and Gradescope. Please make sure you think carefully about the choices.**\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q6_single_tweet) ###\n",
                "\n",
                "# TODO look at the output above to identify which topic the tweet above is most associated to\n",
                "topic_index = tweets_projected[index].argmax()\n",
                "q6 = topic_index"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 7** Largest Topic\n",
                "In our analysis above where we modeled each tweet in 5 topics, which topic has the most tweets strongly associated with it? \n",
                "\n",
                "For each tweet, calculate which topic it is most strongly associated with by looking at the topic values for the tweet. If there is ever a tie for the largest topic weight, take the one with the lowest index (although this is unlikely to happen in our dataset).\n",
                "\n",
                "Save the index of the topic with the most tweets strongly associated with it in a variable called `largest_topic`. The result should be an integer for the index of the largest topic.\n",
                "\n",
                "*Hint: There is a very efficient way to do this using code like we wrote in HW6, but there are many ways to solve this problem in general*.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q7_largest_topic) ###\n",
                "\n",
                "# TODO find index of largest topic\n",
                "most_associated_topics = tweets_projected.argmax(axis=1)\n",
                "topic_counts = np.bincount(most_associated_topics)\n",
                "\n",
                "largest_topic = topic_counts.argmax()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Investigating Trends\n",
                "\n",
                "One benefit of using matrix factorization to a small dimension, is it lets us visualize tweets in this \"topic space\" to find any interesting groupings. \n",
                "\n",
                "Now in our earlier analysis, we modeled each tweet as 5 topics but that is hard to visualize. \n",
                "\n",
                "In the next cell, we will make a new NMF model and projected tweets (called `nmf_small` and `tweets_projected_small` respectively) with 3 components instead of 5. Use the same settings for the other parameters as we did earlier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "nmf_small = NMF(n_components=3, init='nndsvd')\n",
                "tweets_projected_small = nmf_small.fit_transform(tf_idf)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can investigate the topics in this small model. Unsurprisingly, they seem mostly the same but a couple topics had to merge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_top_words(nmf_small.components_, feature_names, 10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have 3 values for each tweet, we can actually plot each tweet in 3D space to see how all the tweets relate to each other. The following cell does exactly that. You don't need to understand all the specifics of how to make a 3D plot, but just note it is using the 3 topic values for each tweet as the x, y, z coordinates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "# Set up axes to plot on\n",
                "fig = plt.figure()\n",
                "ax = fig.add_subplot(projection='3d')\n",
                "\n",
                "# Make 3D scatterplot\n",
                "ax.scatter(tweets_projected_small[:, 0], tweets_projected_small[:, 1], tweets_projected_small[:, 2])\n",
                "\n",
                "# Set axis labels\n",
                "ax.set_xlabel('Topic 0')\n",
                "ax.set_ylabel('Topic 1')\n",
                "ax.set_zlabel('Topic 2')\n",
                "\n",
                "# Rotate plot to be easily viewed\n",
                "ax.view_init(30, 30)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Interesting, it looks like there is a small cluster of Tweets that are far away from all the others when looking at Topic 2. In other words, there are a few tweets that are very far in the Topic 2 direction while the majority of tweets are spread out more in Topic 0/1.\n",
                "\n",
                "### üîç **Question 8** Outlier Tweets\n",
                "Let's look into the tweets that seem very different than the rest. \n",
                "\n",
                "For this problem, we want you to compute all of the unique tweets (since there are some duplicates) that appear in this region in the \"topic space\". \n",
                "\n",
                "Below, we explain the steps to do this computation. Save your result in a variable called `outlier_tweets` that has type `numpy.array` and stores all the unique tweets that are these outliers (as described below).\n",
                "\n",
                "For this problem, you should follow these steps:\n",
                "1. Find which rows in our `tweets_projected_small` are outliers. Based on our visual analysis above, we will define these tweets as ones that have a value of `0.15` or more for Topic 2.\n",
                "2. Now that we know which rows are outliers, use that information to access the `text` column of our original tweets DataFrame (also called `text`) for those rows.\n",
                "3. Use the `.unique` function (available on a column of a pandas DataFrame) to find all the unique values (e.g., ignore copy-paste tweets).\n",
                "\n",
                "If you follow these steps (particularly the last), you will end up with a `numpy.array` with all of the unique tweets that are outliers. Note that many of the tweets look similar, but they count as unique tweets since they have some character differences!\n",
                "\n",
                "Do you spot a theme amongst these tweets? Do you think there is an explanation why our model isolated them as their own topic?\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q8_outlier_tweets) ###\n",
                "\n",
                "# TODO implement the process explained above\n",
                "outlier_rows = tweets_projected_small[:, 2] \u003e= 0.15\n",
                "\n",
                "outlier_tweets = text[outlier_rows]['text']\n",
                "\n",
                "outlier_tweets = outlier_tweets.unique()"
            ]
        }
    ]
}
